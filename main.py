from dotenv import load_dotenv
load_dotenv()

from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_cohere import CohereRerank

from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from langgraph.graph import END, StateGraph, START
from langchain_qdrant import FastEmbedSparse
from langchain.schema import Document
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from fastapi.middleware.cors import CORSMiddleware

from database import QdrantVT
from router import QueryRouter
from tools import ToolSearch
from database.chat_history import ChatHistoryManager

from typing_extensions import TypedDict
from typing import List

from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
from typing import Optional
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi import HTTPException, Request, Form

from pprint import pprint
from database.create_docs import extract_text_from_pdf, LawDocumentCreator
import os
import json
import asyncio

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],  # Cho ph√©p origin c·ªßa giao di·ªán
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

### Kh·ªüi t·∫°o c√°c CONSTANT ###
#HUGGINGFACE_MODEL = "intfloat/multilingual-e5-base"
HUGGINGFACE_MODEL = 'keepitreal/vietnamese-sbert'
FAST_EMBED_SPARSE_MODEL = "Qdrant/bm42-all-minilm-l6-v2-attentions"
# LLM_MODEL = "llama-3.1-8b-instant"
LLM_MODEL = "sonar-pro"
COLLECTION_NAME = "law_collection"
VN_LAW_PDF_PATH = "VanBanGoc_52.2014.QH13.pdf"


### Kh·ªüi t·∫°o ChatHistoryManager ###
DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "user": os.getenv("DB_USER", "root"),
    "password": os.getenv("DB_PASSWORD", "hoangducdung"),
    "database": os.getenv("DB_NAME", "law_db")
}

chat_history_manager = ChatHistoryManager(**DB_CONFIG)

### Kh·ªüi t·∫°o c√°c ƒë·ªëi t∆∞·ª£ng ###
embeddings = HuggingFaceEmbeddings(model_name=HUGGINGFACE_MODEL)
sparse = FastEmbedSparse(model_name=FAST_EMBED_SPARSE_MODEL, batch_size=4)
collection = COLLECTION_NAME

###  ###
qdrant_vectors = QdrantVT(collectionName=collection, embeddingModel=embeddings, SparseModel=sparse)
# llm = ChatGroq(model_name=LLM_MODEL, api_key="")
# llm = ChatOpenAI(
#     api_key="",
#     base_url="https://api.perplexity.ai",
#     model_name = LLM_MODEL,
#     streaming=True
# )
# query_router = QueryRouter(llm=llm)
# --- Model c·∫•u h√¨nh ---
# D√πng .env thay v√¨ hardcode key


from langchain_openai import ChatOpenAI

llm_nostream = ChatOpenAI(
    api_key="",
    base_url="https://api.perplexity.ai",
    model_name=LLM_MODEL,
    streaming=False,
    temperature=0,
)

llm_stream = ChatOpenAI(
    api_key="",
    base_url="https://api.perplexity.ai",
    model_name=LLM_MODEL,
    streaming=True,
    temperature=0,
)


# QueryRouter ph·∫£i nh·∫≠n model non-stream
query_router = QueryRouter(llm=llm_nostream)

tool = ToolSearch()
wiki_tool = tool.wiki
documents = qdrant_vectors.load_documents(pdf_path=VN_LAW_PDF_PATH)
try:
    qdrant_vectors.client.get_collection(collection) 
    retriever = qdrant_vectors.load_vectorstore()

    
except:
    retriever = qdrant_vectors.init_vectorstore()
    
compressor =  CohereRerank(cohere_api_key="", model="rerank-multilingual-v3.0")
rerank_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)

### FUNCTIONS ###
class GraphState(TypedDict):
    """
    TraÃ£ng thaÃÅi cuÃâa ƒë√¥ÃÄ thiÃ£

    Attributes:
        question: c√¢u h·ªèi
        session_id: ID c·ªßa session ƒë·ªÉ l·∫•y context
        instruction: h∆∞·ªõng d·∫´n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        generation: LLM generation
        documents: danh s√°ch c√°c vƒÉn b·∫£n
        answer: c√¢u tr·∫£ l·ªùi
    """
    question: str
    session_id: str  # Th√™m field n√†y
    instruction: str
    generation: str
    documents: List[str]
    answer: str

def retrieve(state):
    print("---RETRIEVE---")
    instruction = state["instruction"]
    question = state["question"]    
    documents = rerank_retriever.invoke(instruction)
    print(f"Number of documents retrieved: {len(documents)}")
    
    text = ""
    
    for i, result in enumerate(documents):
        try:
            print(f"Document {i} metadata keys: {list(result.metadata.keys())}")
            print(f"Document {i} metadata: {result.metadata}")
            
            # Safely get metadata with default values using .get()
            chapter_number = result.metadata.get('chapter_number', 'N/A')
            chapter_title = result.metadata.get('chapter_title', 'N/A')
            article_number = result.metadata.get('article_number', 'N/A')
            
            print(f"Extracted - Chapter: {chapter_number}, Title: {chapter_title}, Article: {article_number}")
            
            # Format the text with safe metadata access
            if chapter_number != 'N/A' and chapter_title != 'N/A' and article_number != 'N/A':
                # Full metadata available
                text += f"{i+1}. {chapter_number} - {chapter_title} - {article_number}\n {result.page_content}\n\n"
            else:
                # Missing some metadata, use a simpler format
                text += f"{i+1}. {article_number if article_number != 'N/A' else 'ƒêi·ªÅu kh√¥ng x√°c ƒë·ªãnh'}\n {result.page_content}\n\n"
                
        except Exception as e:
            print(f"Error processing document {i}: {str(e)}")
            print(f"Document type: {type(result)}")
            print(f"Has metadata attr: {hasattr(result, 'metadata')}")
            if hasattr(result, 'metadata'):
                print(f"Metadata type: {type(result.metadata)}")
            # Fallback to simple format
            text += f"{i+1}. N·ªôi dung ph√°p l√Ω\n {result.page_content}\n\n"
    
    print("---RETRIEVE COMPLETED---")
    return {"documents": text, "question": question}

def wiki_search(state):

    print("---WIKI_SEARCH---")
    question = state["question"]
    wiki_results = wiki_tool.invoke({"query": question})
    wiki_results = Document(page_content=wiki_results)
    return {"documents": wiki_results, "question": question}

def route_question(state):
    
    print("---ROUTE_QUESTION---")
    question = state["question"]
    source = query_router.route_question(question)

    if source.datasource == "wiki_search":
        print("---ROUTE QUESTION TO WIKI SEARCH---")
        return "wiki_search"
    elif source.datasource == "vectorstore":
        print("---ROUTE QUESTION TO VECTORSTORE---")
        return "vectorstore"

def preprocess_query(state):
    session_id = state.get('session_id')
    conversation_context = ""

    if session_id:
        try:
            # L·∫•y 5 tin nh·∫Øn g·∫ßn nh·∫•t ƒë·ªÉ c√≥ context
            context_messages = chat_history_manager.get_conversation_context(session_id, 5)
            
            if context_messages:
                conversation_context = "\n--- NG·ªÆ C·∫¢NH CU·ªòC TR√í CHUY·ªÜN TR∆Ø·ªöC ƒê√ì ---\n"
                for msg in context_messages[:-1]:  # Lo·∫°i b·ªè tin nh·∫Øn hi·ªán t·∫°i
                    role = "üë§ Ng∆∞·ªùi d√πng" if msg['role'] == 'user' else "ü§ñ Tr·ª£ l√Ω"
                    conversation_context += f"{role}: {msg['content'][:200]}{'...' if len(msg['content']) > 200 else ''}\n"
                conversation_context += "--- K·∫æT TH√öC NG·ªÆ C·∫¢NH ---\n\n"
        except Exception as e:
            print(f"L·ªói khi l·∫•y context: {e}")
            conversation_context = ""

    prompt = f"""
    B·∫°n l√† chuy√™n gia Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh Vi·ªát Nam 2014. Khi nh·∫≠n ƒë∆∞·ª£c m·ªôt t√¨nh hu·ªëng ho·∫∑c c√¢u h·ªèi, h√£y:

    Nhi·ªám v·ª• c·ªßa b·∫°n:
    1. **ƒê·ªçc ng·ªØ c·∫£nh cu·ªôc tr√≤ chuy·ªán** (n·∫øu c√≥) ƒë·ªÉ hi·ªÉu m·∫°ch c√¢u chuy·ªán v√† c√°c v·∫•n ƒë·ªÅ ƒë√£ ƒë∆∞·ª£c th·∫£o lu·∫≠n.
    2. **Ph√¢n t√≠ch c√¢u h·ªèi hi·ªán t·∫°i** trong b·ªëi c·∫£nh c·ªßa cu·ªôc tr√≤ chuy·ªán.
    3. **T√≥m t·∫Øt n·ªôi dung ch√≠nh** c·ªßa t√¨nh hu·ªëng (bao g·ªìm c·∫£ context tr∆∞·ªõc ƒë√≥ n·∫øu li√™n quan).
    4. **Tr√≠ch xu·∫•t t·ª´ kh√≥a quan tr·ªçng** t·ª´ t√¨nh hu·ªëng v√† 5 t·ª´ kh√≥a t∆∞∆°ng t·ª± c√πng √Ω nghƒ©a.
    5. **Li·ªát k√™ c√°c ƒëi·ªÅu lu·∫≠t** c√≥ th·ªÉ √°p d·ª•ng.

    Ch·ªâ tr·∫£ l·ªùi v·ªõi 3 m·ª•c cu·ªëi (t√≥m t·∫Øt, t·ª´ kh√≥a, ƒëi·ªÅu lu·∫≠t), kh√¥ng gi·∫£i th√≠ch th√™m.

    V√≠ d·ª•:
    T√¨nh hu·ªëng: "Hai v·ª£ ch·ªìng ƒë·ªìng √Ω ly h√¥n v√† ƒë√£ th·ªèa thu·∫≠n xong vi·ªác chia t√†i s·∫£n."
    Tr·∫£ l·ªùi:
    T√≥m t·∫Øt: Thu·∫≠n t√¨nh ly h√¥n, ƒë√£ th·ªèa thu·∫≠n chia t√†i s·∫£n.
    T·ª´ kh√≥a: Thu·∫≠n t√¨nh ly h√¥n, chia t√†i s·∫£n, ly h√¥n, th·ªèa thu·∫≠n, t√†i s·∫£n.
    ƒêi·ªÅu lu·∫≠t: ƒêi·ªÅu 55. Thu·∫≠n t√¨nh ly h√¥n.

    ƒê√¢y l√† ng·ªØ c·∫£nh:
    {conversation_context}
    ƒê√¢y l√† c√¢u h·ªèi d√†nh cho b·∫°n: {state['question']}
    """

    
    response = llm_stream.invoke(prompt)
    
    return {
        "instruction": response.content
    }

async def chatbot_stream(state):
    """Chatbot v·ªõi streaming response"""
    session_id = state.get('session_id')
    conversation_context = ""

    if session_id:
        try:
            context_messages = chat_history_manager.get_conversation_context(session_id, 5)
            if context_messages:
                conversation_context = "\n--- NG·ªÆ C·∫¢NH CU·ªòC TR√í CHUY·ªÜN TR∆Ø·ªöC ƒê√ì ---\n"
                for msg in context_messages[:-1]:
                    role = "üë§ Ng∆∞·ªùi d√πng" if msg['role'] == 'user' else "ü§ñ Tr·ª£ l√Ω"
                    content = msg.get('content', '')
                    conversation_context += f"{role}: {content[:200]}{'...' if len(content) > 200 else ''}\n"
                conversation_context += "--- K·∫æT TH√öC NG·ªÆ C·∫¢NH ---\n\n"
        except Exception as e:
            print(f"L·ªói khi l·∫•y context: {e}")
            conversation_context = ""

    prompt = f"""
    B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o th√¥ng minh, chuy√™n s√¢u v·ªÅ Lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh Vi·ªát Nam.
    B·∫°n c√≥ kh·∫£ nƒÉng truy c·∫≠p v√†o c√°c t√†i li·ªáu ph√°p l√Ω li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß nh·∫•t.

    T√†i li·ªáu ph√°p l√Ω: {state.get('documents', '')}
    {conversation_context}
    C√¢u h·ªèi: {state.get('question', '')}

    L∆∞u √Ω: ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi d·ª±a tr√™n c√°c quy ƒë·ªãnh v√† ƒëi·ªÅu kho·∫£n trong c√°c t√†i li·ªáu ph√°p l√Ω.
    """

    full_response = ""
    # D√πng stream() ho·∫∑c astream() (n·∫øu mu·ªën async th·ª±c th·ª•)
    for chunk in llm_stream.stream(prompt):
        text = getattr(chunk, "content", None) or str(chunk)
        full_response += text
        yield text

    # N·∫øu mu·ªën l∆∞u full_response ƒë·ªÉ truy xu·∫•t sau:
    state["answer"] = full_response
    return  # ‚ùó KH√îNG return gi√° tr·ªã trong async generator

        
def chatbot(state):

    session_id = state.get('session_id')
    conversation_context = ""

    if session_id:
        try:
            # L·∫•y 5 tin nh·∫Øn g·∫ßn nh·∫•t ƒë·ªÉ c√≥ context
            context_messages = chat_history_manager.get_conversation_context(session_id, 5)
            
            if context_messages:
                conversation_context = "\n--- NG·ªÆ C·∫¢NH CU·ªòC TR√í CHUY·ªÜN TR∆Ø·ªöC ƒê√ì ---\n"
                for msg in context_messages[:-1]:  # Lo·∫°i b·ªè tin nh·∫Øn hi·ªán t·∫°i
                    role = "üë§ Ng∆∞·ªùi d√πng" if msg['role'] == 'user' else "ü§ñ Tr·ª£ l√Ω"
                    conversation_context += f"{role}: {msg['content'][:200]}{'...' if len(msg['content']) > 200 else ''}\n"
                conversation_context += "--- K·∫æT TH√öC NG·ªÆ C·∫¢NH ---\n\n"
        except Exception as e:
            print(f"L·ªói khi l·∫•y context: {e}")
            conversation_context = ""

    # Prompt c·∫£i ti·∫øn cho t√°c v·ª• RAG
    prompt = f"""
    B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o th√¥ng minh, chuy√™n s√¢u v·ªÅ Lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh Vi·ªát Nam. 
    B·∫°n c√≥ kh·∫£ nƒÉng truy c·∫≠p v√†o c√°c t√†i li·ªáu ph√°p l√Ω li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß nh·∫•t.

    D∆∞·ªõi ƒë√¢y l√† t√†i li·ªáu ph√°p l√Ω m√† b·∫°n c√≥ th·ªÉ tham kh·∫£o ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. 
    Vui l√≤ng ph√¢n t√≠ch context, c√¢u h·ªèi v√† s·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu ƒë·ªÉ cung c·∫•p c√¢u tr·∫£ l·ªùi r√µ r√†ng v√† ch√≠nh x√°c.

    T√†i li·ªáu ph√°p l√Ω: {state['documents']}

    {conversation_context}

    C√¢u h·ªèi: {state['question']}

    L∆∞u √Ω: ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi d·ª±a tr√™n c√°c quy ƒë·ªãnh v√† ƒëi·ªÅu kho·∫£n trong c√°c t√†i li·ªáu ph√°p l√Ω, kh√¥ng th√™m th√¥ng tin ngo√†i t√†i li·ªáu.
    """
    
    # G·ª≠i prompt v√† c√¢u h·ªèi v√†o LLM ƒë·ªÉ nh·∫≠n c√¢u tr·∫£ l·ªùi
    response = llm_stream.invoke(prompt)
    
    return {
        "answer": [response]
    }


workflow = StateGraph(GraphState)
# ƒê·ªãnh nghƒ©a c√°c node
workflow.add_node("preprocess_query", preprocess_query)
workflow.add_node("wiki_search", wiki_search)  # web search
workflow.add_node("retrieve", retrieve)  # retrieve
workflow.add_node("chatbot", chatbot)  # chatbot
# preprocess_query

# ƒê·ªãnh nghƒ©a c√°c c·∫°nh
# X√¢y d·ª±ng ƒë·ªì th·ªã
# workflow.add_edge(START, "preprocess_query")
workflow.add_conditional_edges(
    START,
    route_question,
    {
        "wiki_search": "wiki_search",
        "vectorstore": "preprocess_query",
    },
)
workflow.add_edge( "preprocess_query", "retrieve")
workflow.add_edge( "retrieve", "chatbot")
workflow.add_edge( "wiki_search", "chatbot")
workflow.add_edge( "chatbot", END)

# Compile
graph = workflow.compile()

try:
    graph_image = graph.get_graph().draw_mermaid_png()
    with open("workflow_graph.png", "wb") as f:
        f.write(graph_image)
    print("\nƒê√£ l∆∞u ƒë·ªì th·ªã workflow v√†o file 'workflow_graph.png'")
except Exception as e:
    print(f"\nKh√¥ng th·ªÉ t·∫°o ƒë·ªì th·ªã: {str(e)}")


# MODELS
class UserCreate(BaseModel):
    username: str
    email: Optional[str] = None

class SessionCreate(BaseModel):
    user_id: str
    title: Optional[str] = "New Chat"

class QuestionInput(BaseModel):
    user_id: str
    session_id: Optional[str] = None
    question: str

class SessionUpdate(BaseModel):
    title: str

class MessageSearch(BaseModel):
    user_id: str
    query: str

import json, asyncio, traceback
from datetime import datetime
from langchain.schema import Document

async def gen_streaming_response(inputs: QuestionInput):
    """H√†m t·∫°o response streaming (SSE) an to√†n, c√≥ log l·ªói chi ti·∫øt."""
    # helper: g·ª≠i event SSE
    async def send(evt: dict):
        try:
            yield f"data: {json.dumps(evt, ensure_ascii=False)}\n\n"
        except Exception as e:
            # n·∫øu serialize l·ªói (do value kh√¥ng jsonable) ‚Üí g·ª≠i fallback
            safe_evt = {k: str(v) for k, v in evt.items()}
            yield f"data: {json.dumps(safe_evt, ensure_ascii=False)}\n\n"

    try:
        # --- B1: chu·∫©n b·ªã session ---
        session_id = inputs.session_id
        if not session_id:
            session_id = chat_history_manager.create_session(inputs.user_id, "New Chat")
            if not session_id:
                async for chunk in send({"type": "error", "error": "Kh√¥ng th·ªÉ t·∫°o session"}):
                    yield chunk
                return

        async for chunk in send({"type": "session_id", "session_id": session_id}):
            yield chunk

        # --- B2: routing ngu·ªìn d·ªØ li·ªáu ---
        question_text = inputs.question
        try:
            source = query_router.route_question(question_text)  # pydantic model
            datasource = getattr(source, "datasource", "vectorstore")
        except Exception as e:
            print("[route_question error]", e)
            datasource = "vectorstore"  # fallback an to√†n

        # --- B3: l·∫•y conversation context (d√πng cho c·∫£ 2 nh√°nh) ---
        conversation_context = ""
        try:
            context_messages = chat_history_manager.get_conversation_context(session_id, 5)
            if context_messages:
                conversation_context = "\n--- NG·ªÆ C·∫¢NH CU·ªòC TR√í CHUY·ªÜN TR∆Ø·ªöC ƒê√ì ---\n"
                for msg in context_messages[:-1]:
                    role = "üë§ Ng∆∞·ªùi d√πng" if msg.get('role') == 'user' else "ü§ñ Tr·ª£ l√Ω"
                    content = (msg.get('content') or "")[:200]
                    conversation_context += f"{role}: {content}{'...' if len(msg.get('content') or '') > 200 else ''}\n"
                conversation_context += "--- K·∫æT TH√öC NG·ªÆ C·∫¢NH ---\n\n"
        except Exception as e:
            print("[context error]", e)
            conversation_context = ""

        # --- B4: chu·∫©n b·ªã t√†i li·ªáu ---
        documents_text = ""
        if datasource == "wiki_search":
            async for chunk in send({"type": "status", "message": "ƒêang t√¨m ki·∫øm tr√™n Wikipedia..."}):
                yield chunk
            try:
                wiki_result = wiki_tool.invoke({"query": question_text})  # string expected
                documents_text = Document(page_content=wiki_result).page_content or ""
            except Exception as e:
                tb = traceback.format_exc()
                print("[wiki_search error]\n", tb)
                async for chunk in send({"type": "error", "error": f"Wiki search l·ªói: {str(e)}"}):
                    yield chunk
                # v·∫´n ti·∫øp t·ª•c, nh∆∞ng documents_text r·ªóng
        else:
            async for chunk in send({"type": "status", "message": "ƒêang t√¨m ki·∫øm trong c∆° s·ªü d·ªØ li·ªáu..."}):
                yield chunk

            preprocess_prompt = f"""
            B·∫°n l√† chuy√™n gia Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh Vi·ªát Nam 2014.
            H√£y ph√¢n t√≠ch ng·ªØ c·∫£nh (n·∫øu c√≥) v√† c√¢u h·ªèi ƒë·ªÉ t·∫°o ch·ªâ d·∫´n/truy v·∫•n t√¨m t√†i li·ªáu ng·∫Øn g·ªçn, ch√≠nh x√°c.

            {conversation_context}
            C√¢u h·ªèi: {question_text}
            """.strip()

            try:
                instruction = llm_stream.invoke(preprocess_prompt).content
            except Exception as e:
                tb = traceback.format_exc()
                print("[preprocess llm error]\n", tb)
                async for chunk in send({"type": "error", "error": f"L·ªói LLM preprocess: {str(e)}"}):
                    yield chunk
                instruction = question_text  # fallback

            async for chunk in send({"type": "status", "message": "Retrieving t√†i li·ªáu..."}):
                yield chunk

            try:
                documents = rerank_retriever.invoke(instruction)
                cnt = len(documents) if hasattr(documents, "__len__") else 0
                async for chunk in send({"type": "documents_retrieved", "count": cnt}):
                    yield chunk

                buf = []
                for i, result in enumerate(documents or []):
                    try:
                        md = getattr(result, "metadata", {}) or {}
                        chapter_number = md.get('chapter_number', 'N/A')
                        chapter_title  = md.get('chapter_title', 'N/A')
                        article_number = md.get('article_number', 'N/A')
                        if chapter_number != 'N/A' and chapter_title != 'N/A' and article_number != 'N/A':
                            buf.append(f"{i+1}. {chapter_number} - {chapter_title} - {article_number}\n{result.page_content}\n")
                        else:
                            label = article_number if article_number != 'N/A' else 'ƒêi·ªÅu kh√¥ng x√°c ƒë·ªãnh'
                            buf.append(f"{i+1}. {label}\n{result.page_content}\n")
                    except Exception:
                        buf.append(f"{i+1}. N·ªôi dung ph√°p l√Ω\n{getattr(result, 'page_content', '')}\n")
                documents_text = "\n".join(buf)
            except Exception as e:
                tb = traceback.format_exc()
                print("[retriever error]\n", tb)
                async for chunk in send({"type": "error", "error": f"L·ªói retrieve: {str(e)}"}):
                    yield chunk
                documents_text = ""

        # --- B5: stream c√¢u tr·∫£ l·ªùi ---
        async for chunk in send({"type": "generating", "message": "ƒêang gen c√¢u tr·∫£ l·ªùi..."}):
            yield chunk

        final_prompt = f"""
        B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o th√¥ng minh, chuy√™n s√¢u v·ªÅ Lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh Vi·ªát Nam.
        H√£y d·ª±a ch·∫∑t ch·∫Ω v√†o t√†i li·ªáu ph√°p l√Ω b√™n d∆∞·ªõi ƒë·ªÉ tr·∫£ l·ªùi r√µ r√†ng, c√≥ tr√≠ch d·∫´n ƒëi·ªÅu lu·∫≠t khi ph√π h·ª£p.

        T√†i li·ªáu ph√°p l√Ω:
        {documents_text}

        {conversation_context}
        C√¢u h·ªèi: {question_text}
        """.strip()

        full_answer = ""
        try:
            # Quan tr·ªçng: d√πng ƒë√∫ng API streaming
            for chunk in llm_stream.stream(final_prompt):  # ‚ùó ch·ªâ stream ·ªü ƒë√¢y
                text = getattr(chunk, "content", "") or ""
                if text:
                    full_answer += text
                    yield f"data: {json.dumps({'type':'content','content': text}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.02)  # nh·ªãp cho UI m∆∞·ª£t
        except Exception as e:
            tb = traceback.format_exc()
            print("[llm stream error]\n", tb)
            async for chunk in send({"type": "error", "error": f"L·ªói stream LLM: {str(e)}"}):
                yield chunk

        # --- B6: l∆∞u l·ªãch s·ª≠ + metadata ---
        try:
            if full_answer.strip():
                chat_history_manager.add_message(session_id, "assistant", full_answer)

            context_metadata = {
                "last_question": question_text,
                "last_answer": full_answer[:500] + "..." if len(full_answer) > 500 else full_answer,
                "timestamp": datetime.now().isoformat(),
                "user_id": inputs.user_id,
            }
            _ = chat_history_manager.save_session_context(session_id, context_metadata)
        except Exception as e:
            print("[save history error]", e)

        async for chunk in send({"type": "completed"}):
            yield chunk

    except Exception as e:
        tb = traceback.format_exc()
        print("[stream outer error]\n", tb)
        async for chunk in send({"type": "error", "error": f"{str(e) or 'Unknown error'}", "traceback": tb}):
            yield chunk


from fastapi.responses import StreamingResponse

@app.post("/api/chat/stream")
async def chat_stream(inputs: QuestionInput):
    # Tr·∫£ v·ªÅ async generator
    return StreamingResponse(
        gen_streaming_response(inputs),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


    return JSONResponse(content=event_generator(), media_type="text/event-stream")

@app.post("/api/users")
async def create_user(user: UserCreate):
    """T·∫°o user m·ªõi"""
    try:
        user_id = chat_history_manager.create_user(user.username, user.email)
        if user_id:
            return {"status": "success", "user_id": user_id}
        else:
            raise HTTPException(status_code=400, detail="Kh√¥ng th·ªÉ t·∫°o user")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/users/{username}")
async def get_user_by_username(username: str):
    """L·∫•y th√¥ng tin user theo username"""
    try:
        user = chat_history_manager.get_user_by_username(username)
        if user:
            return {"status": "success", "user": user}
        else:
            raise HTTPException(status_code=404, detail="User not found")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/users/{user_id}/sessions")
async def get_user_sessions(user_id: str, limit: int = 50):
    """L·∫•y danh s√°ch sessions c·ªßa user"""
    try:
        sessions = chat_history_manager.get_user_sessions(user_id, limit)
        return {"status": "success", "sessions": sessions}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/api/sessions")
async def create_session(session: SessionCreate):
    """T·∫°o session m·ªõi"""
    try:
        session_id = chat_history_manager.create_session(session.user_id, session.title)
        if session_id:
            return {"status": "success", "session_id": session_id}
        else:
            raise HTTPException(status_code=400, detail="Kh√¥ng th·ªÉ t·∫°o session")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/sessions/{session_id}/messages")
async def get_session_messages(session_id: str, limit: int = 100):
    """L·∫•y messages c·ªßa session"""
    try:
        messages = chat_history_manager.get_session_messages(session_id, limit)
        return {"status": "success", "messages": messages}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/sessions/{session_id}")
async def update_session(session_id: str, session_update: SessionUpdate):
    """C·∫≠p nh·∫≠t title session"""
    try:
        success = chat_history_manager.update_session_title(session_id, session_update.title)
        if success:
            return {"status": "success", "message": "Session updated successfully"}
        else:
            raise HTTPException(status_code=400, detail="Kh√¥ng th·ªÉ c·∫≠p nh·∫≠t session")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/sessions/{session_id}")
async def delete_session(session_id: str):
    """X√≥a session"""
    try:
        success = chat_history_manager.delete_session(session_id)
        if success:
            return {"status": "success", "message": "Session deleted successfully"}
        else:
            raise HTTPException(status_code=400, detail="Kh√¥ng th·ªÉ x√≥a session")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))




# Chat Endpoints
@app.post("/api/chat")
async def chat(inputs: QuestionInput):
    """Chat v·ªõi bot v√† l∆∞u l·ªãch s·ª≠"""
    try:
        # N·∫øu kh√¥ng c√≥ session_id, t·∫°o session m·ªõi
        session_id = inputs.session_id
        if not session_id:
            session_id = chat_history_manager.create_session(inputs.user_id, "New Chat")
            if not session_id:
                raise HTTPException(status_code=400, detail="Kh√¥ng th·ªÉ t·∫°o session")

        # L·∫•y conversation context hi·ªán t·∫°i (ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng trong t∆∞∆°ng lai)
        conversation_context = chat_history_manager.get_conversation_context(session_id, 10)

        # L∆∞u c√¢u h·ªèi c·ªßa user
        user_message_id = chat_history_manager.add_message(
            session_id, "user", inputs.question
        )
        
        # X·ª≠ l√Ω c√¢u h·ªèi v·ªõi graph
        results = graph.invoke({
            "question": inputs.question,
            "session_id": session_id
        })
        #answer = results.get('answer', '')

        raw_answer = results.get('answer', '')
        if isinstance(raw_answer, list) and len(raw_answer) > 0:
            # N·∫øu answer l√† list, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n
            answer_obj = raw_answer[0]
            if hasattr(answer_obj, 'content'):
                # N·∫øu l√† AIMessage, l·∫•y content
                answer = answer_obj.content
            else:
                answer = str(answer_obj)
        elif hasattr(raw_answer, 'content'):
            # N·∫øu tr·ª±c ti·∫øp l√† AIMessage
            answer = raw_answer.content
        else:
            # Fallback to string conversion
            answer = str(raw_answer)
        
        print(f"[DEBUG] Raw answer type: {type(raw_answer)}")
        print(f"[DEBUG] Processed answer: {answer[:100]}...")
        
        # L∆∞u c√¢u tr·∫£ l·ªùi c·ªßa assistant
        assistant_message_id = chat_history_manager.add_message(
            session_id, "assistant", answer
        )

        from datetime import datetime

        context_metadata = {
            "last_question": inputs.question,
            "last_answer": answer[:500] + "..." if len(answer) > 500 else answer,
            "conversation_length": len(conversation_context) + 2,
            "timestamp": datetime.now().isoformat(),
            "user_id": inputs.user_id,
            "processing_results": {
                #"route_used": "vectorstore" if "vectorstore" in str(graph) else "wiki_search",
                "has_documents": "documents" in results,
                "result_keys": list(results.keys()) if isinstance(results, dict) else []
            }
        }
        
        context_saved = chat_history_manager.save_session_context(session_id, context_metadata)
        print(f"Context saved: {context_saved}")

        # T·ª± ƒë·ªông c·∫≠p nh·∫≠t title session n·∫øu ƒë√¢y l√† tin nh·∫Øn ƒë·∫ßu ti√™n
        session_messages = chat_history_manager.get_session_messages(session_id, 5)
        if len(session_messages) == 2:  # Ch·ªâ c√≥ 2 tin nh·∫Øn (user + assistant)
            # T·∫°o title ng·∫Øn g·ªçn t·ª´ c√¢u h·ªèi
            title = inputs.question[:50] + "..." if len(inputs.question) > 50 else inputs.question
            chat_history_manager.update_session_title(session_id, title)
        
        return {
            "status": "success",
            "session_id": session_id,
            "answer": answer,
            "user_message_id": user_message_id,
            "assistant_message_id": assistant_message_id,
            "context_saved": context_saved,
            "conversation_context_lenght": len(conversation_context),
        }
    except Exception as e:
        print(f"Error in chat endpoint: {str(e)}")
        print(f"Error type: {type(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/sessions/{session_id}/context")
async def get_conversation_context(session_id: str, last_n_messages: int = 10):
    """L·∫•y context conversation"""
    try:
        context = chat_history_manager.get_conversation_context(session_id, last_n_messages)
        return {"status": "success", "context": context}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/api/sessions/{session_id}/save_context")
async def save_session_context_endpoint(session_id: str, context_data: dict):
    """Test endpoint ƒë·ªÉ l∆∞u context"""
    try:
        success = chat_history_manager.save_session_context(session_id, context_data)
        if success:
            return {"status": "success", "message": "Context saved"}
        else:
            return {"status": "error", "message": "Failed to save context"}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/api/sessions/{session_id}/statistics")
async def get_session_statistics(session_id: str):
    """L·∫•y th·ªëng k√™ session"""
    try:
        stats = chat_history_manager.get_session_statistics(session_id)
        return {"status": "success", "statistics": stats}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
# Search Endpoints
@app.post("/api/search/messages")
async def search_messages(search: MessageSearch):
    """T√¨m ki·∫øm messages"""
    try:
        messages = chat_history_manager.search_messages(search.user_id, search.query)
        return {"status": "success", "messages": messages}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# @app.post("/api/chat")
# async def chat(inputs: QuestionInput):
#     # results = []
    
#     # # Gi·∫£ s·ª≠ b·∫°n ƒë√£ c√≥ `app.stream()` cho ph√©p l·∫•y c√°c output t·ª´ c√¢u h·ªèi.
#     # for output in graph.invoke({"question": inputs.question}):
#     #     output_result = {}
#     #     for key, value in output.items():
#     #         output_result[key] = value
#     #     results.append(output_result)
#     results = graph.invoke({"question": inputs.question})
#     answer = results.get('answer','')
#     #save_chat_history(inputs.user_id, inputs.question, answer)
#     return {"results": results}

# PDF Upload Endpoint
@app.post("/api/upload_pdf")
async def upload_pdf(file: UploadFile = File(...)):
    # L∆∞u file t·∫°m th·ªùi
    file_location = f"temp_{file.filename}"
    with open(file_location, "wb") as f:
        f.write(await file.read())
    # 1. Tr√≠ch xu·∫•t text
    raw_text = extract_text_from_pdf(file_location)
    # 2. Kh·ªüi t·∫°o class thao t√°c DB
    db_config = {
        "host": "localhost",
        "user": "root",
        "password": "hoangducdung",
        "database": "law_db"
    }
    doc_creator = LawDocumentCreator(**db_config)
    # 3. L∆∞u v√†o MySQL
    doc_creator.add_chapters_and_articles_to_db(raw_text)
    # 4. T·∫°o Document t·ª´ MySQL
    documents = doc_creator.create_documents_from_db()
    # 5. Upsert v√†o Qdrant
    qdrant_vectors.upsert_documents(documents)
    return {"status": "success", "filename": file.filename}

# Health Check
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "message": "Law Chat API is running"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="localhost", port=8000)